# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t1HPQJmGAcYwZnwP6oSOfy-0VJrDe_Ns
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from data_generator import SimonDataGenerator
from model import SimonDistinguisher


def train_model():
    # --- Hyperparameters ---
    ROUNDS = 7         # Attack 11 rounds (Paper achieves ~99% here, easier for testing)
    BATCH_SIZE = 5000      # Large batch size is good for this statistical task
    EPOCHS = 10            # Paper uses 30, but 20 is enough to see convergence
    LEARNING_RATE = 0.001  # Standard Adam LR
    SAMPLES_PER_EPOCH = 100000 # Number of samples to generate per epoch

    # Check device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Training on device: {device}")

    # --- 1. Initialize Components ---
    generator = SimonDataGenerator()
    net = SimonDistinguisher(input_size=64).to(device)

    # Loss and Optimizer
    # Paper uses MSE (Mean Squared Error) for regression-like training
    # but BCE (Binary Cross Entropy) is often more stable for classification (0 or 1).
    # Let's use MSE to stick to the paper's methodology[cite: 648].
    criterion = nn.MSELoss()
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, weight_decay=1e-5) # L2 reg mentioned in paper [cite: 649]

    # Learning Rate Scheduler (Cyclic) [cite: 651]
    # Simple StepLR is easier for a start, but we can mimic the paper's cyclic nature using OneCycleLR or similar.
    # For simplicity, we'll use a standard scheduler that reduces LR when loss plateaus.
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)

    # --- 2. Training Loop ---
    print(f"Starting training for Simon 32/64 ({ROUNDS} Rounds)...")

    for epoch in range(EPOCHS):
        net.train()
        running_loss = 0.0
        correct_predictions = 0
        total_predictions = 0

        # Generate fresh data for this epoch (On-the-fly generation avoids overfitting)
        X_np, Y_np = generator.generate_batch(batch_size=SAMPLES_PER_EPOCH, rounds=ROUNDS)

        # Convert to Binary Features
        X_bin = generator.convert_to_binary(X_np)

        # Convert to PyTorch Tensors
        X_tensor = torch.from_numpy(X_bin).to(device)
        Y_tensor = torch.from_numpy(Y_np).to(device)

        # Create DataLoader for mini-batching
        dataset = TensorDataset(X_tensor, Y_tensor)
        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

        for inputs, labels in train_loader:
            # Zero gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = net(inputs)

            # Loss calculation
            loss = criterion(outputs, labels)

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            # Statistics
            running_loss += loss.item()

            # Accuracy Calculation (Threshold 0.5)
            predicted = (outputs > 0.5).float()
            correct_predictions += (predicted == labels).sum().item()
            total_predictions += labels.size(0)

        # Epoch Metrics
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = correct_predictions / total_predictions

        # Update Scheduler
        scheduler.step(epoch_loss)

        print(f"Epoch [{epoch+1}/{EPOCHS}] "
              f"Loss: {epoch_loss:.4f} | "
              f"Acc: {epoch_acc:.4f} ({(epoch_acc*100):.2f}%)")

        # --- Validation Step (Optional but recommended) ---
        # Generate a small validation set to check generalization
        if (epoch + 1) % 5 == 0:
            validate(net, generator, ROUNDS, device)

    print("Training Complete.")
    return net

def validate(net, generator, rounds, device):
    net.eval()
    val_size = 10000
    with torch.no_grad():
        X_val, Y_val = generator.generate_batch(batch_size=val_size, rounds=rounds)
        X_bin_val = generator.convert_to_binary(X_val)

        X_tensor = torch.from_numpy(X_bin_val).to(device)
        Y_tensor = torch.from_numpy(Y_val).to(device)

        outputs = net(X_tensor)
        predicted = (outputs > 0.5).float()
        accuracy = (predicted == Y_tensor).sum().item() / val_size

        print(f"--- Validation Accuracy (Round {rounds}): {accuracy:.4f} ---")

# --- EXECUTE ---
if __name__ == "__main__":
    trained_model = train_model()

    # Save the model
    torch.save(trained_model.state_dict(), "simon_distinguisher.pth")
    print("Model saved to simon_distinguisher.pth")